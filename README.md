# üß† AI Document Question Answering System (RAG)

A **local, privacy-first AI system** that allows users to upload PDF documents and ask questions grounded strictly in the document content using **Retrieval-Augmented Generation (RAG)**.

This project was built to deeply understand how **modern AI document assistants work internally**, not just to consume APIs.

---

## üöÄ What This Project Does

- Upload any **PDF document**
- Automatically:
  - Extract text from the PDF
  - Split it into overlapping chunks
  - Convert chunks into vector embeddings
  - Store embeddings in a **FAISS vector database**
- Ask natural language questions
- Receive **accurate, context-grounded answers**
- Runs **fully locally** using lightweight LLMs via **Ollama**
- Uses **document-level caching** to avoid repeated computation

---

## üß† Why This Project Matters

This is **not just a chatbot**.

It is a **real Retrieval-Augmented Generation (RAG) system**, the same architecture used in:

- Enterprise document search
- Internal AI knowledge assistants
- AI copilots
- Research and legal document analysis tools

The project demonstrates:
- Understanding of **LLM limitations**
- How to **augment models with external knowledge**
- Efficient **vector search using FAISS**
- Performance optimization using **caching**
- Clean and modular system design

---

## üèóÔ∏è Architecture Overview

PDF Upload
‚Üì
PDF Loader
‚Üì
Text Chunking (with overlap)
‚Üì
Embedding Generation
‚Üì
FAISS Vector Store (Similarity Search)
‚Üì
Relevant Context Retrieval
‚Üì
Prompt Construction
‚Üì
Local LLM (Ollama)
‚Üì
Final Answer (Grounded in Document)

---

## üìÅ Project Structure

ai-doc-assistant/
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ ‚îú‚îÄ‚îÄ pdf_loader.py # Extracts text from PDFs
‚îÇ ‚îú‚îÄ‚îÄ chunking.py # Splits text into overlapping chunks
‚îÇ ‚îú‚îÄ‚îÄ embeddings.py # Generates vector embeddings
‚îÇ ‚îú‚îÄ‚îÄ vector_store.py # FAISS indexing and retrieval
‚îÇ ‚îú‚îÄ‚îÄ llm.py # Local LLM interface (Ollama)
‚îÇ ‚îú‚îÄ‚îÄ evaluation.py # Optional answer evaluation
‚îÇ ‚îî‚îÄ‚îÄ rag.py # Orchestrates the RAG pipeline
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ ‚îú‚îÄ‚îÄ docs/ # Uploaded PDFs
‚îÇ ‚îî‚îÄ‚îÄ cache/ # Cached FAISS indexes
‚îÇ
‚îú‚îÄ‚îÄ ui.py # Streamlit UI
‚îú‚îÄ‚îÄ main.py # CLI testing entry point
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt

---

## üß© Key Design Decisions

### ‚úÖ Chunking with Overlap
- Prevents loss of context
- Avoids broken sentences
- Improves embedding quality

### ‚úÖ FAISS Vector Database
- Extremely fast similarity search
- Scales well to large documents
- Industry-standard vector search library

### ‚úÖ Document-Based Caching
- Uses a hash of the PDF file
- Prevents re-chunking and re-embedding
- Makes repeated queries near-instant

### ‚úÖ Local LLM Inference
- No API costs
- No data leakage
- Works offline
- Memory-aware model selection

### ‚úÖ Grounded Answers Only
- If the answer is **not in the document**, the model responds:
  > "I don‚Äôt know"
- Prevents hallucinations

---

## üñ•Ô∏è User Interface

Built using **Streamlit**:
- PDF upload interface
- Question input field
- Answer display
- Expandable retrieved context section
- Optional evaluation/debug output

The UI is intentionally minimal and functional.

---

## ‚öôÔ∏è How to Run the Project

### 1Ô∏è‚É£ Install Dependencies
```bash
pip install -r requirements.txt

###2Ô∏è‚É£ Start Ollama and Pull a Model
ollama pull tinyllama:1.1b
ollama run tinyllama:1.1b
(Model can be changed depending on available RAM)

3Ô∏è‚É£ Launch the App
streamlit run ui.py
Open your browser at:
http://localhost:8501
